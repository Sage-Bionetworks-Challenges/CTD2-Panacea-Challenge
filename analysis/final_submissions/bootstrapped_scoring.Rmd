---
title: "Bootstrap analysis of top performers"
author: "Robert Allaway, Sage Bionetworks"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    code_fold: hide
---

In order to declare top-performers for a DREAM challenge, we need to assess if there are any "tied" methods, that is, methods that are not substantially different in performance. We determine this using a bootstrapping (sampling with replacement) approach to determing how a submission would score in different scenarios (that is - when only considering resampled sets of the values to be predicted). For the CTD^2^ challenge, we can do this by sampling the compounds for which targets had to be predicted. Specifically, we sample with replacement all of the submitted predictions and the gold standard and score the prediction files. We repeat this for at total of 1000 samples to obtain a distribution of scores for each participant. We then calculate a Bayes factor relative to the best-scoring method, to see if any of the other methods are within a certain threshold. Smaller Bayes factors indicate more similar performance while larger Bayes factors indicate more disparate performance. We use a Bayes factor of 3 as a cutoff to indicate a tie. 

Unfold the code if you want to see how we do this. 

First, import packages for data manipulation and retrieve prediction data, gold standard data, and the template for use in the scoring code. Also, define the fraction overlap function for calculating SC1 scores.
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
library(magrittr)
library(readr)
library(tidyr)
library(purrr)
library(ggplot2)
library(synapser)
library(forcats)
synLogin()

set.seed(27651030)

query <- synTableQuery('select * from syn21628283')$asDataFrame()

prediction_paths <- sapply(query$id, function(x){
  synGet(x)$path
})


get_user_or_team_names <- function(id){
 name <- try(synGetTeam(id)$name, silent = T) ##try to get the team name from id 
 if(class(name)=='try-error'){ ##if it is not a team, will return error, so then try to get user profile
 try({
   prof <- synGetUserProfile(id = id) ##get first and last name
   fn <- prof$firstName
   ln <- prof$lastName
   if(is.null(fn) | is.null(ln)){
     un <- prof$userName
     return(un) 
   }else if(fn == "" | ln == ""){ ##if empty, get username instead
     un <- prof$userName
     return(un)
   }else{
     return(paste(fn, ln))
   }
   })
   }else{
     return(name)
   }
}

query <- mutate(query, participant_name = sapply(userId, get_user_or_team_names)) %>% 
      mutate(participant_name = case_when(participant_name == "Hyunmin Kim" ~ "Theragen", 
                                participant_name != "Hyunmin Kim" ~ participant_name)) 

prediction_paths <- setNames(prediction_paths, query$participant_name)

 frac_overlap <- function(gold, pred){
  sum(gold %in% pred)/length(gold)
}

gold_path <- synGet("syn21302164")$path
template_path <- synGet('syn21321426')$path


```

Define the targets that we have reliable information on and will score based on.

```{r echo=TRUE, message=FALSE, warning=FALSE}

targs <- c("EGFR","CSNK2A2","BMP2K","AAK1","Q6ZSR9","GAK","PRKD2","PRKD3","SIK2","SIK3","PLK4","AURKA","AURKB","PTK2",
             "FER","PTK2B","TNK1","TNK2","MAP2K5","MAP2K1","MAP2K2","PDGFRB","STK10","SLK","MAP4K2","MAP4K5","MAP4K3",
             "MET","CDK2","CDK5","CDK16","CDK9","GSK3A","GSK3B","CDK7","ERCC2","CDK1","CDK12","CDK13","CLK1","DYRK1A",
             "CDK17","CDK4","CDK6","FGFR1","MAPK10","MAPK8","MAPK9","ADCK1","STK16","NEK9","FECH","CSNK1A1","CSNK1E",
             "CSNK1D","MAP3K4","WEE1","MST1R","PIP4K2C","IRAK1","TAOK3","PIM1","TAOK1","CIT","PRKX","PRKY","CDC42BPA",
             "CDC42BPB","PRKACA","PRKACB","PRKCQ","CLK4","PAK6","PRKACG","RPS6KB1","MAPK7","PRKG1","PRKCI","CDKL5","CDK18",
             "ADRBK1","STK11","EPHB3","BRAF","NLK","TGFBR2","CSK","MYLK","FGR","TESK1","KIT","ULK1","FES","IGF1R","INSR",
             "JAK2","STK26","CAMK4","TAOK2","EIF2AK1","ICK","CSNK1G2","CSNK1G1","CSNK1G3","CLK2","TP53RK","MINK1","MAPK15",
             "ERN1","MAP2K6","MAP3K5","MAP3K6","MAP2K3","ERN2","MAPK1","MAPK3","AKT3","AKT1","AKT2","RPS6KA5","ACVRL1",
             "MAPKAPK5","NEK1","ARAF","ACAD10","ADCK3","ACVR2B","PAK2","CLK3","MAPKAPK3","PLK1","PDXK","DCK","ADK","PKMYT1",
             "NUAK2","HSP90AB2P","STK24","CMPK1","GARS","ACOX3","ACAD11","DCTPP1","NEK7","SMC2","TOP2B","MYH10","PIM2",
             "CAMK1G","PIP4K2A","PIP4K2B","AK2","NME2","CDC42BPG","AP1G1","DHCR24","SLC25A5","ACTR3","SNRNP200",
             "CARS","DNAJA1","PFKP","MAT2A","IPO7","STK38L","AIMP1","CDC7","PRKAR2A","EPHA1","EPHA7","BMPR1B","BMPR1A",
             "TYK2","BMPR2","MAP3K2","MAP3K3","SYK","NEK3","BUB1","LATS1","MAP3K11","IKBKE","TBK1","STK4","STK3","CDK3",
             "MARK4","MELK","IRAK4","RPS6KA4","RPS6KA1","RPS6KA3","MARK2","MARK3","RPS6KA6","PRKAG2","PRKAA1","PRKAG1","PAK4",
             "CHEK1","NTRK1","CAMK2D","CAMK2G","CAMKK2","PHKG2","MAP4K4","TNIK","MYLK3","MAP4K1","JAK1","ULK3","PRKCD",
             "PRKCA","PRKCB","PKN1","PKN2","ROCK2","ROCK1","NQO2","ACVR1","IRAK3","FLT3","RET","DDR1","DDR2","ABL2","BCR",
             "ABL1","RIPK2","MAPKAPK2","MAPK11","MAPK14","ZAK","RIPK3","YES1","LCK","SRC","FYN","HCK","LYN","FRK","EPHA2",
             "EPHA5","EPHB2","EPHB4","EPHA4","BTK","TEC","MAP3K1","LIMK1","LIMK2","PTK6","EPHB6","ACVR1B","TGFBR1")


```

Read in prediction files, slice out top 10 predictions for each compound, and nest dataframes. Then bootstrap the predictions, a gold standard, and a null model 1000 times to calculate 1000 p-values per prediction. 

```{r echo=TRUE, message=FALSE, warning=FALSE}

  gold <- suppressMessages(read_csv(gold_path)) %>% filter(target %in% targs)
  

  template <- read_csv(template_path) %>% filter(target %in% targs)
  
  ###SC1 
  
  gold_df <- gold %>% 
    select(-cmpd) %>% 
    group_by(cmpd_id) %>% 
    nest() %>% 
    arrange(cmpd_id)
  
  pred_df<- lapply(names(prediction_paths), function(x){
    read_csv(prediction_paths[[x]]) %>% 
      gather(cmpd_id, confidence ,-target) %>% 
      filter(target %in% targs) %>% 
      group_by(cmpd_id) %>% 
      arrange(-confidence, target) %>% 
      slice(1:10) %>%  ##instead of top n. We eliminate ties alphabetically!
      nest() %>% 
      arrange(cmpd_id) %>% 
      rename({{x}} := data)
  }) %>% reduce(left_join, by = 'cmpd_id')
  

  sc1_vals <- sapply(1:10000, function(x){
    
    null_model <- template %>% 
      gather(cmpd_id, confidence ,-target) %>% 
      group_by(cmpd_id) %>% 
      arrange(-confidence, target) %>% 
      sample_n(10, replace = F) %>% 
      select(-confidence) %>%
      set_names(c('target', 'cmpd_id')) %>% 
      filter(target %in% targs) %>%
      nest() %>% 
      ungroup() %>% 
      mutate(cmpd_id= 1:32) %>% 
      rename(null = data)
  
  
   join <- inner_join(gold_df, pred_df, by = 'cmpd_id') %>% 
    ungroup() %>% 
     sample_n(32, replace = T) %>% 
     mutate(cmpd_id= 1:32) %>% 
     inner_join(null_model) %>% 
     select(-cmpd_id) 
   
   the_names <- colnames(join)
   
   join <- join %>% 
     map(., function(a){
       map2(.$data, a, function(x,y){
       frac_overlap(x$target, y$target) 
       }) %>% plyr::ldply()
     }) %>% bind_cols
   
   colnames(join) <- the_names

   ps <- join %>% 
     apply(., 2, function(x){
       wilcox.test(x = x, y= .$null, paired = T, exact = NULL)$p.value
       }) 
    
    
    if(any(is.nan(ps))){
      ps[is.nan(ps)] <- 1
    }
    ps
    
  }) %>% t()
  
  # sc1 <- mean(-log2(sc1_vals)) %>% signif(5)
  
  sc1 <- sc1_vals %>% 
    as_data_frame() %>% 
    gather(submission, bs_score)
  
```


Repeat, but for SC2, which is the significance of the target ranks as compared to a null model.


```{r echo=TRUE, message=FALSE, warning=FALSE}

  ##SC2

   
   pred_df<- lapply(names(prediction_paths), function(x){
     read_csv(prediction_paths[[x]]) %>% 
       gather(cmpd_id, confidence ,-target) %>% 
       filter(target %in% targs) %>% 
       group_by(cmpd_id) %>% 
       arrange(-confidence, target) %>%  ##instead of top n. We eliminate ties alphabetically!
       nest() %>% 
       arrange(cmpd_id) %>% 
       rename({{x}} := data)
   }) %>% reduce(left_join, by = 'cmpd_id')
  
  sc2_vals <- sapply(1:10000, function(x){
    
    null_model <- template %>% 
      gather(cmpd_id, confidence ,-target) %>% 
      group_by(cmpd_id) %>% 
      arrange(-confidence, target) %>% 
      sample_frac(1, replace = F) %>% 
      select(-confidence) %>%
      set_names(c('target', 'cmpd_id')) %>% 
      filter(target %in% targs) %>%
      nest() %>% 
      ungroup() %>% 
      mutate(cmpd_id= 1:32) %>% 
      rename(null = data)
    
    join <- inner_join(gold_df, pred_df, by = 'cmpd_id') %>% 
      ungroup() %>% 
      sample_n(32, replace = T) %>% 
      mutate(cmpd_id= 1:32) %>% 
      inner_join(null_model) %>% 
      select(-cmpd_id) 
    
    the_names <- colnames(join)
    
    join <- join %>% 
      map(., function(a){
        map2(.$data, a, function(x,y){
          match(x$target, y$target) %>% unique() 
        }) %>% unlist()
      }) %>% bind_rows
    
    
    colnames(join) <- the_names
    
    ps <- join %>% 
      apply(., 2, function(x){
        wilcox.test(x = x, y= .$null, paired = T, exact = NULL)$p.value
      }) 
    
    
    if(any(is.nan(ps))){
      ps[is.nan(ps)] <- 1
    }
    ps
    
  }) %>% t()
  
   sc2 <- sc2_vals %>% 
    as_data_frame() %>% 
    gather(submission, bs_score)

```

Use our `challengescoring` package to compute Bayes factors using a matrix of scores for SC1 and SC2, setting the `refPredIndex` as the number of the column that contains the top prediction (the reference prediction). 

```{r echo=TRUE, message=FALSE, warning=FALSE}
   
  sc1_bf <- challengescoring::computeBayesFactor(bootstrapMetricMatrix = sc1_vals, refPredIndex = 5, invertBayes = F) %>% 
    as_tibble(rownames = "submission") %>% 
    rename(bayes = value) %>% 
  mutate(submission = case_when(submission == "Bence Szalai" ~ "Baseline", 
                                submission != "Bence Szalai" ~ submission))
  
  sc2_bf <- challengescoring::computeBayesFactor(bootstrapMetricMatrix = sc2_vals, refPredIndex = 10, invertBayes = F) %>% 
    as_tibble(rownames = "submission") %>% 
    rename(bayes = value) %>% 
  mutate(submission = case_when(submission == "Bence Szalai" ~ "Baseline", 
                                submission != "Bence Szalai" ~ submission))
  
```

Then plot SC1 results. Red indicates the reference model, orange indicates a Bayes factor of 3-5, grey indicates a Bayes factor of >5.

```{r echo=TRUE}
sc1_bf %>% filter(!submission %in% c('data','null')) %>% arrange(bayes)

sc1_final <- sc1 %>% 
    mutate(submission = case_when(submission == "Bence Szalai" ~ "Baseline", 
                                submission != "Bence Szalai" ~ submission)) %>% 
  filter(!submission %in% c('data','null')) %>% 
  left_join(sc1_bf) %>% 
  mutate(bayes_category = case_when(bayes == 0 ~ 'Reference',
                                    bayes<=3 ~ '<3',
                                    bayes>=3 & bayes <5 ~ "3-5",
                                    bayes>=5 & bayes <10 ~ "5-10",
                                    bayes>=10 ~ ">10"))
            
ggplot(sc1_final) +
    geom_boxplot(aes(x = fct_reorder(submission, -bs_score, .fun = mean), y = -log2(bs_score), color = bayes_category), outlier.shape = NA) +
    theme_bw() + 
  scale_color_manual(values = c("Reference"="#d32e36", '<3' = '#cf4d6f', "3-5" = "#cc7e85",
                                "5-10" = '#c5afa4', ">10" = "#a8a6a4"),
                      name = "Bayes Factor") +
  coord_flip() +
  labs(y = 'Bootstrapped SC1 score', x = 'Submission')

```

Looking at these results, the top performer is not tied with any other valid submission.

Then plot SC2 results. Red indicates the reference model, orange indicates a Bayes factor of 3-5, grey indicates a Bayes factor of >5.

```{r echo=TRUE}
sc2_bf %>% filter(!submission %in% c('data','null')) %>% arrange(bayes)

sc2_final <- sc2 %>% 
  mutate(submission = case_when(submission == "Bence Szalai" ~ "Baseline", 
                                submission != "Bence Szalai" ~ submission)) %>% 
  filter(!submission %in% c('data','null')) %>% 
  left_join(sc2_bf) %>% 
  mutate(bayes_category = case_when(bayes == 0 ~ 'Reference',
                                    bayes<=3 ~ '<3',
                                    bayes>=3 & bayes <5 ~ "3-5",
                                    bayes>=5 & bayes <10 ~ "5-10",
                                    bayes>=10 ~ ">10"))
ggplot(sc2_final) +
    geom_boxplot(aes(x = fct_reorder(submission, -bs_score, .fun = median), y = -log2(bs_score), color = bayes_category), outlier.shape = NA) +
  theme_bw() + 
  scale_color_manual(values = c("Reference"="#d32e36", '<3' = '#cf4d6f', "3-5" = "#cc7e85",
                                "5-10" = '#c5afa4', ">10" = "#a8a6a4"),
                      name = "Bayes Factor") +
  coord_flip() +
  labs(y = 'Bootstrapped SC2 score', x = 'Submission')


```

Looking at these results, the top performer is not tied with any other valid submission, though one comes close to a tie. 


_______

# Comparison to baseline model

Use our `challengescoring` package to compute Bayes factors using a matrix of scores for SC1 and SC2, setting the `refPredIndex` as the number of the column that contains the *baseline* prediction (the reference prediction). 

```{r echo=TRUE, message=FALSE, warning=FALSE}

computeBayesFactorWhereRefIsNotBest <- function(bootstrapMetricMatrix,
                               refPredIndex,
                               invertBayes){

    M <- as.data.frame(bootstrapMetricMatrix - bootstrapMetricMatrix[,refPredIndex])
    K <- apply(M ,2, function(x) {
      k <- sum(x >= 0)/sum(x < 0)
      if(sum(x >= 0) > sum(x < 0)){
      return(k)
      }else{
      return(1/k)
      }
    })
    if(invertBayes == T){K <- 1/K}
    K[refPredIndex] <- 0

    return(K)
}
   
  sc1_bf <- computeBayesFactorWhereRefIsNotBest(bootstrapMetricMatrix = sc1_vals, refPredIndex = 12, invertBayes = F) %>% 
    as_tibble(rownames = "submission") %>% 
    rename(bayes = value) %>% 
  mutate(submission = case_when(submission == "Bence Szalai" ~ "Baseline", 
                                submission != "Bence Szalai" ~ submission))
  
  sc2_bf <- computeBayesFactorWhereRefIsNotBest(bootstrapMetricMatrix = sc2_vals, refPredIndex = 12, invertBayes = F) %>% 
  as_tibble(rownames = "submission") %>% 
    rename(bayes = value) %>% 
  mutate(submission = case_when(submission == "Bence Szalai" ~ "Baseline", 
                                submission != "Bence Szalai" ~ submission))
```

Then plot SC1 results. Red indicates the reference model, dark pink indicates a Bayes factor of <3, pink: 3-5, silver-pink: 5-10, grey: >10.

```{r echo=TRUE}
sc1_bf %>% filter(!submission %in% c('data','null')) %>% arrange(bayes)

sc1_final <- sc1 %>% 
  mutate(submission = case_when(submission == "Bence Szalai" ~ "Baseline", 
                                submission != "Bence Szalai" ~ submission)) %>% 
  filter(!submission %in% c('data','null')) %>% 
  left_join(sc1_bf) %>% 
  mutate(bayes_category = case_when(bayes == 0 ~ 'Reference',
                                    bayes<=3 ~ '<3',
                                    bayes>=3 & bayes <5 ~ "3-5",
                                    bayes>=5 & bayes <10 ~ "5-10",
                                    bayes>=10 ~ ">10"))
            
ggplot(sc1_final) +
    geom_boxplot(aes(x = fct_reorder(submission, -bs_score, .fun = mean), y = -log2(bs_score), color = bayes_category), outlier.shape = NA) +
    theme_bw() + 
  scale_color_manual(values = c("Reference"="#d32e36", '<3' = '#cf4d6f', "3-5" = "#cc7e85",
                                "5-10" = '#c5afa4', ">10" = "#a8a6a4"),
                      name = "Bayes Factor") +
  coord_flip() +
  labs(y = 'Bootstrapped SC1 score', x = 'Submission')

```

Looking at these results, the baseline is tied with netphar and SBNB for this metric,.

Then plot SC2 results. Red indicates the reference model, dark pink indicates a Bayes factor of <3, pink: 3-5, silver-pink: 5-10, grey: >10.

```{r echo=TRUE}
sc2_bf %>% filter(!submission %in% c('data','null')) %>% arrange(bayes)

sc2_final <- sc2 %>% 
  mutate(submission = case_when(submission == "Bence Szalai" ~ "Baseline", 
                                submission != "Bence Szalai" ~ submission)) %>% 
  filter(!submission %in% c('data','null')) %>% 
  left_join(sc2_bf) %>% 
  mutate(bayes_category = case_when(bayes == 0 ~ 'Reference',
                                    bayes<=3 ~ '<3',
                                    bayes>=3 & bayes <5 ~ "3-5",
                                    bayes>=5 & bayes <10 ~ "5-10",
                                    bayes>=10 ~ ">10"))

ggplot(sc2_final) +
    geom_boxplot(aes(x = fct_reorder(submission, -bs_score, .fun = median), y = -log2(bs_score), color = bayes_category), outlier.shape = NA) +
  theme_bw() + 
  scale_color_manual(values = c("Reference"="#d32e36", '<3' = '#cf4d6f', "3-5" = "#cc7e85",
                                "5-10" = '#c5afa4', ">10" = "#a8a6a4"),
                      name = "Bayes Factor") +
  coord_flip() +
  labs(y = 'Bootstrapped SC2 score', x = 'Submission')


```

Looking at these results, the baseline model is tied with Hyunmin Kim (team theragen)


##Top performer validation

Finally, let's validate the prediction files from the top performers' and "honorable mention" Docker containers. We ran their submitted docker containers by hand to generate these prediction files - we'll now verify that they are matched to the submitted prediction files to demonstrate that their submitted codebase is representative of their submitted prediction. 

Given that some of these methods, like neural nets, can use random initiations or have other uncontrollable randomness, we'll use a bootstrapping method like above to determine if the prediction files are equivalent, as they may not be exactly numerically identical. 

```{r echo=TRUE}

atom_docker <- synGet("syn21769349")$path 
atom_sub <- prediction_paths[['Atom']]
  
netphar_docker <- synGet("syn21777036")$path
netphar_sub <- prediction_paths[['netphar']]

sbnb_docker <- synGet("syn21777122")$path 
sbnb_sub <- prediction_paths[['SBNB']]

prediction_paths_docker <- list("atom_docker" = atom_docker,
                         "atom_submission" = atom_sub,
                         "netphar_docker" = netphar_docker,
                         "netphar_submission" = netphar_sub,
                         "sbnb_docker" = sbnb_docker,
                         "sbnb_submission" = sbnb_sub)

template <- read_csv(template_path) %>% filter(target %in% targs)
  
  ###SC1 
  
  gold_df <- gold %>% 
    select(-cmpd) %>% 
    group_by(cmpd_id) %>% 
    nest() %>% 
    arrange(cmpd_id)
  
  pred_df<- lapply(names(prediction_paths_docker), function(x){
    read_csv(prediction_paths_docker[[x]]) %>% 
      gather(cmpd_id, confidence ,-target) %>% 
      filter(target %in% targs) %>% 
      group_by(cmpd_id) %>% 
      arrange(-confidence, target) %>% 
      slice(1:10) %>%  ##instead of top n. We eliminate ties alphabetically!
      nest() %>% 
      arrange(cmpd_id) %>% 
      rename({{x}} := data)
  }) %>% reduce(left_join, by = 'cmpd_id')
  

  sc1_vals <- suppressMessages(sapply(1:1000, function(x){
    
    null_model <- template %>% 
      gather(cmpd_id, confidence ,-target) %>% 
      group_by(cmpd_id) %>% 
      arrange(-confidence, target) %>% 
      sample_n(10, replace = F) %>% 
      select(-confidence) %>%
      set_names(c('target', 'cmpd_id')) %>% 
      filter(target %in% targs) %>%
      nest() %>% 
      ungroup() %>% 
      mutate(cmpd_id= 1:32) %>% 
      rename(null = data)
  
  
   join <- inner_join(gold_df, pred_df, by = 'cmpd_id') %>% 
    ungroup() %>% 
     sample_n(32, replace = T) %>% 
     mutate(cmpd_id= 1:32) %>% 
     inner_join(null_model) %>% 
     select(-cmpd_id) 
   
   the_names <- colnames(join)
   
   join <- join %>% 
     map(., function(a){
       map2(.$data, a, function(x,y){
       frac_overlap(x$target, y$target) 
       }) %>% plyr::ldply()
     }) %>% bind_cols
   
   colnames(join) <- the_names

   ps <- join %>% 
     apply(., 2, function(x){
       suppressWarnings(wilcox.test(x = x, y= .$null, paired = T, exact = NULL)$p.value)
       }) 
    
    
    if(any(is.nan(ps))){
      ps[is.nan(ps)] <- 1
    }
    ps
    
  })) %>% t()
  
  # sc1 <- mean(-log2(sc1_vals)) %>% signif(5)
  
  sc1 <- sc1_vals %>% 
    as_data_frame() %>% 
    gather(submission, bs_score)


```


##Atom 

```{r echo=TRUE}

if(all_equal(sc1_vals[,2],sc1_vals[,3])){
  print('Atom submissions identical')
} else {
  atom_bf <- challengescoring::computeBayesFactor(bootstrapMetricMatrix = sc1_vals[,2:3], refPredIndex = 2, invertBayes = F) %>% 
  as_tibble(rownames = "submission") %>% 
    rename(bayes = value) 
  
  print(atom_bf)
}


```


## netphar

```{r echo=TRUE}

if(all_equal(sc1_vals[,4],sc1_vals[,5])){
  print('netphar submissions identical')
} else {
  netphar_bf <- challengescoring::computeBayesFactor(bootstrapMetricMatrix = sc1_vals[,4:5], refPredIndex = 2, invertBayes = F) %>% 
  as_tibble(rownames = "submission") %>% 
    rename(bayes = value) 
  
  print(netphar_bf)
}


```

#SBNB

```{r echo=TRUE}

if(all_equal(sc1_vals[,6],sc1_vals[,7])){
  print('SBNB submissions identical')
} else {
  sbnb_bf <- challengescoring::computeBayesFactor(bootstrapMetricMatrix = sc1_vals[,6:7], refPredIndex = 2, invertBayes = F) %>% 
  as_tibble(rownames = "submission") %>% 
    rename(bayes = value) 
  
  print(sbnb_bf)
}


```